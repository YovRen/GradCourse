\begin{Verbatim}[commandchars=\\\{\}]

\PYG{c+c1}{\PYGZsh{} 步骤 1: 生成一个回合。}
    \PYG{c+c1}{\PYGZsh{} 一个回合是由（状态，动作，奖励）元组组成的数组}
\PYG{n}{episode} \PYG{o}{=} \PYG{p}{[]}
\PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{()}
\PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} 根据状态选择动作的概率分布}
    \PYG{n}{probs} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]} \PYG{k}{if} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{18} \PYG{k}{else} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} 概率分布}
    \PYG{n}{action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{),} \PYG{n}{p}\PYG{o}{=}\PYG{n}{probs}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} 根据概率选择动作}
    \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{info} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} 执行动作，获取下一个状态、奖励等信息}
    \PYG{n}{episode}\PYG{o}{.}\PYG{n}{append}\PYG{p}{((}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{))}  \PYG{c+c1}{\PYGZsh{} 记录状态、动作和奖励}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
    \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
        \PYG{k}{break}

\PYG{c+c1}{\PYGZsh{} 步骤 2: 找出我们在这个回合中访问过的所有（状态，动作）对}
\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards} \PYG{o}{=} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{o}{*}\PYG{n}{episode}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} 解压回合元组，获取状态、动作和奖励信息}

\PYG{c+c1}{\PYGZsh{} 步骤 3: 计算所有采样回合中该状态的平均回报}
\PYG{n}{discounts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{([}\PYG{n}{discount\PYGZus{}factor}\PYG{o}{**}\PYG{n}{i} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)])}  \PYG{c+c1}{\PYGZsh{} 计算折扣因子}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{states}\PYG{p}{):}
    \PYG{c+c1}{\PYGZsh{} 更新动作值函数}
    \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{state}\PYG{p}{][}\PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]]} \PYG{o}{+=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{:]} \PYG{o}{*} \PYG{n}{discounts}\PYG{p}{[:}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{i} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)])}  \PYG{c+c1}{\PYGZsh{} 计算回报总和}
    \PYG{n}{returns\PYGZus{}count}\PYG{p}{[}\PYG{n}{state}\PYG{p}{][}\PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]]} \PYG{o}{+=} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} 记录访问次数}
    \PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{][}\PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]]} \PYG{o}{=} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{state}\PYG{p}{][}\PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]]} \PYG{o}{/} \PYG{n}{returns\PYGZus{}count}\PYG{p}{[}\PYG{n}{state}\PYG{p}{][}\PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]]}  \PYG{c+c1}{\PYGZsh{} 计算平均值}


\end{Verbatim}

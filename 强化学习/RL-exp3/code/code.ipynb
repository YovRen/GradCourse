{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1').unwrapped\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.shape[0]\n",
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "epsilon = 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPO(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.state_ph = tf.placeholder(tf.float32, [None, state_shape], 'state2')\n",
    "\n",
    "        with tf.variable_scope('value2'):\n",
    "            layer1_v = tf.layers.dense(self.state_ph, 100, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(layer1_v, 1)\n",
    "            self.Q = tf.placeholder(tf.float32, [None, 1], 'discounted_r2')\n",
    "            self.advantage = self.Q - self.v\n",
    "            self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
    "            self.train_value_nw = tf.train.AdamOptimizer(0.002).minimize(self.value_loss)\n",
    "\n",
    "        pi, pi_params = self.build_policy_network('pi2', trainable=True)\n",
    "        oldpi, oldpi_params = self.build_policy_network('oldpi2', trainable=False)\n",
    "\n",
    "        with tf.variable_scope('sample_action2'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)\n",
    "\n",
    "        with tf.variable_scope('update_oldpi2'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.action_ph = tf.placeholder(tf.float32, [None, action_shape], 'action2')\n",
    "        self.advantage_ph = tf.placeholder(tf.float32, [None, 1], 'advantage2')\n",
    "\n",
    "        with tf.variable_scope('loss2'):\n",
    "            with tf.variable_scope('surrogate2'):\n",
    "                ratio = pi.prob(self.action_ph) / oldpi.prob(self.action_ph)\n",
    "                objective = ratio * self.advantage_ph\n",
    "\n",
    "                # TRPO-specific constraint\n",
    "                kl_divergence = tf.distributions.kl_divergence(oldpi, pi)\n",
    "                trpo_loss = -tf.reduce_mean(objective - 0.01 * kl_divergence)\n",
    "\n",
    "            self.policy_loss = trpo_loss\n",
    "\n",
    "        with tf.variable_scope('train_policy2'):\n",
    "            self.global_step = tf.train.get_or_create_global_step()\n",
    "            self.learning_rate = tf.train.exponential_decay(0.001, self.global_step, 10000, 0.96, staircase=True)\n",
    "            self.train_policy_nw = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.policy_loss, global_step=self.global_step)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self, state, action, reward):\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        adv = self.sess.run(self.advantage, {self.state_ph: state, self.Q: reward})\n",
    "        feed_dict = {self.state_ph: state, self.action_ph: action, self.advantage_ph: adv}\n",
    "        [self.sess.run(self.train_policy_nw, {self.state_ph: state, self.action_ph: action, self.advantage_ph: adv}) for _ in range(10)]\n",
    "        [self.sess.run(self.train_value_nw, {self.state_ph: state, self.Q: reward}) for _ in range(10)]\n",
    "\n",
    "\n",
    "    def build_policy_network(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            layer1_pi = tf.layers.dense(self.state_ph, 100, tf.nn.relu, trainable=trainable)\n",
    "            mu = 2 * tf.layers.dense(layer1_pi, action_shape, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(layer1_pi, action_shape, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        action = self.sess.run(self.sample_op, {self.state_ph: state})[0]\n",
    "        action = np.clip(action, action_bound[0], action_bound[1])\n",
    "        return action\n",
    "\n",
    "    def get_state_value(self, state):\n",
    "        if state.ndim < 2:\n",
    "            state = state[np.newaxis, :]\n",
    "        return self.sess.run(self.v, {self.state_ph: state})[0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义PPO类\n",
    "class PPO(object):\n",
    "    def __init__(self):\n",
    "        # 开始TensorFlow会话\n",
    "        self.sess = tf.Session()\n",
    "        # 定义状态的占位符\n",
    "        self.state_ph = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
    "\n",
    "        # 构建值网络，返回状态的值\n",
    "        with tf.variable_scope('value'):\n",
    "            layer1 = tf.layers.dense(self.state_ph, 100, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(layer1, 1)\n",
    "            self.Q = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.advantage = self.Q - self.v\n",
    "            self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
    "            # 使用Adam优化器最小化值网络的损失\n",
    "            self.train_value_nw = tf.train.AdamOptimizer(0.002).minimize(self.value_loss)\n",
    "\n",
    "        # 获取策略和其参数\n",
    "        pi, pi_params = self.build_policy_network('pi', trainable=True)\n",
    "        oldpi, oldpi_params = self.build_policy_network('oldpi', trainable=False)\n",
    "\n",
    "        # 从新策略中抽样一个动作\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)\n",
    "\n",
    "        # 更新旧策略的参数\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        # 定义动作和优势的占位符\n",
    "        self.action_ph = tf.placeholder(tf.float32, [None, action_shape], 'action')\n",
    "        self.advantage_ph = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "\n",
    "        # 定义策略网络的代理目标函数\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "                # 首先，定义比率\n",
    "                ratio = pi.prob(self.action_ph) / oldpi.prob(self.action_ph)\n",
    "                # 通过将比率和优势值相乘来定义目标\n",
    "                objective = ratio * self.advantage_ph\n",
    "                # 使用修剪后的比率和未修剪的目标值定义目标函数\n",
    "                L = tf.reduce_mean(tf.minimum(objective, tf.clip_by_value(ratio, 1.-epsilon, 1.+ epsilon)*self.advantage_ph))\n",
    "\n",
    "            # 计算梯度，并通过使用梯度上升来最大化目标函数。然而，我们可以通过添加负号将上述最大化目标转换为最小化目标。因此，我们可以将策略网络的损失表示为：\n",
    "            self.policy_loss = -L\n",
    "\n",
    "        # 使用Adam优化器最小化策略网络的损失\n",
    "        with tf.variable_scope('train_policy'):\n",
    "            self.train_policy_nw = tf.train.AdamOptimizer(0.001).minimize(self.policy_loss)\n",
    "\n",
    "        # 初始化所有TensorFlow变量\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 训练函数\n",
    "    def train(self, state, action, reward):\n",
    "        # 更新旧策略\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        # 计算优势值\n",
    "        adv = self.sess.run(self.advantage, {self.state_ph: state, self.Q: reward})\n",
    "        # 训练策略网络\n",
    "        [self.sess.run(self.train_policy_nw, {self.state_ph: state, self.action_ph: action, self.advantage_ph: adv}) for _ in range(10)]\n",
    "        # 训练值网络\n",
    "        [self.sess.run(self.train_value_nw, {self.state_ph: state, self.Q: reward}) for _ in range(10)]\n",
    "\n",
    "    # 构建策略网络\n",
    "    def build_policy_network(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            # 定义网络的层\n",
    "            layer1 = tf.layers.dense(self.state_ph, 100, tf.nn.relu, trainable=trainable)\n",
    "            # 计算均值\n",
    "            mu = 2 * tf.layers.dense(layer1, action_shape, tf.nn.tanh, trainable=trainable)\n",
    "            # 计算标准差\n",
    "            sigma = tf.layers.dense(layer1, action_shape, tf.nn.softplus, trainable=trainable)\n",
    "            # 计算正态分布\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "\n",
    "        # 获取策略网络的参数\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "\n",
    "    # 选择动作函数\n",
    "    def select_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        # 从策略网络生成的正态分布中抽样一个动作\n",
    "        action = self.sess.run(self.sample_op, {self.state_ph: state})[0]\n",
    "        # 将动作剪切，使其在动作边界内\n",
    "        action = np.clip(action, action_bound[0], action_bound[1])\n",
    "        return action\n",
    "\n",
    "    # 获取状态值函数\n",
    "    def get_state_value(self, state):\n",
    "        if state.ndim < 2:\n",
    "            state = state[np.newaxis, :]\n",
    "        return self.sess.run(self.v, {self.state_ph: state})[0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpo = TRPO()\n",
    "ppo = PPO()\n",
    "num_episodes = 20\n",
    "num_timesteps = 200\n",
    "gamma = 0.9\n",
    "batch_size = 32\n",
    "def trainer(model):\n",
    "    returns_list = []\n",
    "    #for each episode\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        Return = 0\n",
    "        #for every step\n",
    "        for t in range(num_timesteps):   \n",
    "            #render the environment\n",
    "            env.render()\n",
    "            #select the action\n",
    "            action = model.select_action(state)\n",
    "            #perform the selected action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            #store the state, action, and reward in the list\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append((reward+8)/8)    \n",
    "            #update the state to the next state\n",
    "            state = next_state\n",
    "            #update the return\n",
    "            Return += reward\n",
    "            #if we reached the batch size or if we reached the final step of the episode\n",
    "            if (t+1) % batch_size == 0 or t == num_timesteps-1:\n",
    "                #compute the value of the next state\n",
    "                v_s_ = model.get_state_value(next_state)\n",
    "                #compute Q value as sum of reward and discounted value of next state\n",
    "                discounted_r = []\n",
    "                for reward in episode_rewards[::-1]:\n",
    "                    v_s_ = reward + gamma * v_s_\n",
    "                    discounted_r.append(v_s_)\n",
    "                discounted_r.reverse()\n",
    "                #stack the episode states, actions, and rewards:\n",
    "                es, ea, er = np.vstack(episode_states), np.vstack(episode_actions), np.array(discounted_r)[:, np.newaxis]\n",
    "                #empty the lists\n",
    "                episode_states, episode_actions, episode_rewards = [], [], []\n",
    "                #train the network\n",
    "                model.train(es, ea, er)\n",
    "        #print the return for every 10 episodes\n",
    "        returns_list.append(Return)\n",
    "        if i %10 ==0:\n",
    "            print(\"Episode:{}, Return: {}\".format(i,Return))  \n",
    "    return returns_list\n",
    "\n",
    "returns_trpo = trainer(trpo)\n",
    "returns_ppo = trainer(ppo)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(returns_trpo, label='TRPO')\n",
    "plt.plot(returns_ppo, label='PPO')\n",
    "plt.title('Comparison of TRPO and PPO')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Return')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('torch18')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abd70ae60c8c2833e63aace0b7493006069f8b3061f34fc51f7cc8e79bdf7e24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

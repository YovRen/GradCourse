{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e0d20a-ebb7-4de4-b350-2350de3a02c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa61d248-bdb1-43f0-81b1-39a20dbe86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "hidden_dropout_prob = 0.3\n",
    "num_labels = 2\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-2\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ff2ef8-e45d-4853-b667-c59694158a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.iloc[idx][\"text\"]\n",
    "        label = self.dataset.iloc[idx][\"label\"]\n",
    "        sample = {\"text\": text, \"label\": label}\n",
    "        return sample\n",
    "\n",
    "# 划分数据集\n",
    "path_to_file = \"llm-detect-ai-generated-text/train_v4_drcat_01.csv\"\n",
    "dataset = pd.read_csv(path_to_file, sep=\",\", names=[\"text\",\"label\",\"prompt_name\",\"source\",\"RDizzl3_seven\",\"model\"], skiprows=1)\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices = random.sample(indices, int(0.9 * len(dataset)))\n",
    "test_indices = list(set(indices) - set(train_indices))\n",
    "\n",
    "# 使用方括号而不是圆括号\n",
    "train_set = SentimentDataset(dataset.iloc[train_indices])\n",
    "test_set = SentimentDataset(dataset.iloc[test_indices])\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e007221-f442-47a1-8715-d0c693019c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /gemini/pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /gemini/pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 定义 tokenizer，传入词汇表\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/gemini/pretrain\")\n",
    "config = BertConfig.from_pretrained(\"/gemini/pretrain\", num_labels=num_labels, hidden_dropout_prob=hidden_dropout_prob)\n",
    "model = BertForSequenceClassification.from_pretrained(\"/gemini/pretrain\", config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ed45fe-c953-4cfc-a936-bff4ff0056a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器和损失函数\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "# 设置 bias 和 LayerNorm.weight 不使用 weight_decay\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de38a449-dcd0-4863-bf58-c043df7a1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device, freezeLLM):\n",
    "    if freezeLLM:\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        total_params = sum(p.numel() for p in model.bert.parameters())\n",
    "        print(f\"Freeze parameters in the model: {total_params}\")\n",
    "    else:\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    y_true_train = []\n",
    "    y_scores_train = []\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        label = batch[\"label\"]\n",
    "        text = batch[\"text\"]\n",
    "        tokenized_text = tokenizer(text, max_length=100, add_special_tokens=True, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "        label = label.clone().to(device).detach()\n",
    "        output = model(**tokenized_text, labels=label)\n",
    "        y_pred_prob = output[1]\n",
    "        loss = criterion(y_pred_prob.view(-1, 2), label.view(-1))\n",
    "        y_true_train.extend(label.cpu().numpy())\n",
    "        y_scores_train.extend(y_pred_prob[:, 1].cpu().detach().numpy())  # Assuming 1 is the positive class index\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 每隔一定步数打印当前 loss 和计算 AUC-ROC\n",
    "        if (i+1) % 100 == 0:\n",
    "            auc_train = roc_auc_score(y_true_train, y_scores_train)\n",
    "            print(\"epoch\", i+1, \"\\t\", \"current loss:\", epoch_loss / (i+1), \"auc-roc:\", auc_train)\n",
    "\n",
    "    return epoch_loss / len(iterator), auc_train\n",
    "\n",
    "# 其他部分的代码保持不变\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    y_true_eval = []\n",
    "    y_scores_eval = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            label = batch[\"label\"]\n",
    "            text = batch[\"text\"]\n",
    "            tokenized_text = tokenizer(text, max_length=100, add_special_tokens=True, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "            label = label.clone().to(device).detach()\n",
    "            output = model(**tokenized_text, labels=label)\n",
    "            y_pred_prob = output[1]\n",
    "            y_true_eval.extend(label.cpu().numpy())\n",
    "            y_scores_eval.extend(y_pred_prob[:, 1].cpu().detach().numpy())  # Assuming 1 is the positive class index\n",
    "            loss = output[0]\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    auc_eval = roc_auc_score(y_true_eval, y_scores_eval)\n",
    "    return epoch_loss / len(iterator), auc_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708e855e-e8dc-46d6-b60b-8ae6109b7e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze parameters in the model: 109482240\n",
      "epoch 100 \t current loss: 0.6666945070028305 auc-roc: 0.49045136029462894\n",
      "epoch 200 \t current loss: 0.66067117780447 auc-roc: 0.4971431734240753\n",
      "epoch 300 \t current loss: 0.6570651801427205 auc-roc: 0.5023685652227351\n",
      "epoch 400 \t current loss: 0.6537523891031742 auc-roc: 0.5084049594573892\n",
      "epoch 500 \t current loss: 0.6527262557744979 auc-roc: 0.5136432881052895\n",
      "epoch 600 \t current loss: 0.6510228065649668 auc-roc: 0.5191831221486636\n",
      "epoch 700 \t current loss: 0.6483020538943155 auc-roc: 0.5277478325060045\n",
      "epoch 800 \t current loss: 0.6474259801208972 auc-roc: 0.5315094487464447\n",
      "epoch 900 \t current loss: 0.646144156522221 auc-roc: 0.5358905600004513\n",
      "epoch 1000 \t current loss: 0.6435979158878327 auc-roc: 0.5432108096422851\n",
      "final valid loss:  0.6543782480384992 \t valid auc-roc: 0.7453787793541692\n",
      "epoch 100 \t current loss: 0.324977592676878 auc-roc: 0.9252474636254882\n",
      "epoch 200 \t current loss: 0.26979101125150917 auc-roc: 0.947165167146488\n",
      "epoch 300 \t current loss: 0.24037403824428719 auc-roc: 0.9575856929610118\n",
      "epoch 400 \t current loss: 0.22400083377957344 auc-roc: 0.9625844256379676\n",
      "epoch 500 \t current loss: 0.21097421386092902 auc-roc: 0.9664668082308742\n",
      "epoch 600 \t current loss: 0.199954696095859 auc-roc: 0.9695581867630086\n",
      "epoch 700 \t current loss: 0.19400874544733338 auc-roc: 0.971114897765408\n",
      "epoch 800 \t current loss: 0.1869778544898145 auc-roc: 0.973045567180722\n",
      "epoch 900 \t current loss: 0.18100272074962656 auc-roc: 0.9747646702273566\n",
      "epoch 1000 \t current loss: 0.17756859241239725 auc-roc: 0.9756949333763196\n",
      "final valid loss:  0.7719211489670789 \t valid auc-roc: 0.9632797433180487\n",
      "epoch 100 \t current loss: 0.1164447531849146 auc-roc: 0.9891419358588912\n",
      "epoch 200 \t current loss: 0.1147510916274041 auc-roc: 0.9893078178200727\n",
      "epoch 300 \t current loss: 0.11794997157528997 auc-roc: 0.9887324191245345\n",
      "epoch 400 \t current loss: 0.11644669684581459 auc-roc: 0.9890307458659653\n",
      "epoch 500 \t current loss: 0.11339104428142309 auc-roc: 0.9896985570241372\n",
      "epoch 600 \t current loss: 0.11131051138043403 auc-roc: 0.9900977786095186\n",
      "epoch 700 \t current loss: 0.11014124088122376 auc-roc: 0.9902996882916438\n",
      "epoch 800 \t current loss: 0.10931689440039918 auc-roc: 0.9904249560318041\n",
      "epoch 900 \t current loss: 0.1083178462036368 auc-roc: 0.9906914042655485\n",
      "epoch 1000 \t current loss: 0.10753226787876338 auc-roc: 0.9907889695993805\n",
      "final valid loss:  0.7640502816347061 \t valid auc-roc: 0.9760653144916529\n"
     ]
    }
   ],
   "source": [
    "# 开始训练和验证\n",
    "for i in range(epochs):\n",
    "    if i==0:\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, freezeLLM=True)\n",
    "    else:\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, freezeLLM=False)\n",
    "    valid_loss, valid_acc = evaluate(model, test_loader, device)\n",
    "    print(\"final valid loss: \", valid_loss, \"\\t\", \"valid auc-roc:\", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b656a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "# Test data processing\n",
    "test_inputs = tokenizer(test['text'].tolist(), padding=True, return_tensors='pt')\n",
    "\n",
    "# Move input tensor to the same device as the model\n",
    "test_inputs = {key: value.to(device) for key, value in test_inputs.items()}\n",
    "\n",
    "# Generate predictions using your trained model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Assuming the first column of logits corresponds to the negative class (non-AI-generated) \n",
    "# and the second column corresponds to the positive class (AI-generated)\n",
    "predictions = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # Move predictions back to CPU\n",
    "\n",
    "# Create a submission DataFrame with essay IDs and corresponding predictions\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'generated': predictions\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('torch18')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "abd70ae60c8c2833e63aace0b7493006069f8b3061f34fc51f7cc8e79bdf7e24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

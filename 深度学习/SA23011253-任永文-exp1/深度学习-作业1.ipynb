{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e596c951",
   "metadata": {},
   "source": [
    "# <div align='center'>实验一</div>\n",
    "### <div align='right'>SA23011253 任永文</div>\n",
    "\n",
    "## 实验要求\n",
    "\n",
    "使用 pytorch 或者 tensorflow 手写一个前馈神经网络，用于近似函数：\n",
    "\n",
    "$$\n",
    "y = \\sin(x) , \\quad x \\in [0, 2\\pi)\n",
    "$$\n",
    "\n",
    "并研究网络深度、学习率、网络宽度、激活函数对模型性能的影响。\n",
    "\n",
    "## 实验步骤\n",
    "\n",
    "1. **网络框架**：要求选择 pytorch 或 tensorflow 其中之一，依据官方网站的指引安装包。若你需要使用 GPU，可能还需安装 CUDA 驱动。本次实验仅利用 CPU 也可以完成，但仍强烈推荐大家安装 GPU 版本，以满足后续实验需求。\n",
    "2. **数据生成**：本次实验的数据集仅需使用程序自动生成，即在 $[0, 2\\pi)$ 范围内随机 sample 样本作为 $x$ 值，并计算 $y$ 值。要求生成三个**互不相交**的数据集分别作为**训练集、验证集、测试集**。训练只能在训练集上完成，实验调参只能在验证集上完成。\n",
    "3. **模型搭建**：采用 pytorch 或 tensorflow 所封装的 module 编写模型，例如 torch.nn.Linear(), torch.nn.Relu() 等，无需手动完成底层 forward、backward 过程。\n",
    "4. **模型训练**：将生成的训练集输入搭建好的模型进行前向的 loss 计算和反向的梯度传播，从而训练模型，同时也建议使用网络框架封装的 optimizer 完成参数更新过程。训练过程中记录模型在训练集和验证集上的损失，并绘图可视化。\n",
    "5. **调参分析**：将训练好的模型在验证集上进行测试，以 **Mean Square Error(MSE)** 作为网络性能指标。然后，对网络深度、学习率、网络宽度、激活函数等模型超参数进行调整，再重新训练、测试，并分析对模型性能的影响。\n",
    "6. **测试性能**：选择你认为最合适的（例如，在验证集上表现最好的）一组超参数，重新训练模型，并在测试集上测试（注意，这理应是你的实验中**唯一**一次在测试集上的测试），并记录测试的结果（MSE）。\n",
    "\n",
    "## 实验提交\n",
    "\n",
    "本次实验截止日期为 **<mark>11 月 1 日 23:59:59</mark>**，提交到邮箱 ustcdl2023@163.com ，具体要求如下：\n",
    "\n",
    "1. 全部文件打包在一个压缩包内，压缩包命名为 学号- 姓名 - exp1.zip\n",
    "2. 代码仅包含 .py 文件，请勿包含实验中间结果（例如中间保存的数据集等），如果有多个文件，放在 src/ 文件夹内。\n",
    "3. 代码中提供一个可以直接运行的并输出结果的 **main.py**，结果包括训练集损失、验证集损失随 epoch 改变的曲线（保存下来）和测试集的 MSE。\n",
    "4. 代码中提供一个描述所有需依赖包的 requirements.txt，手动列入代码中用到的所有非标准库及版本或者使用 `pip freeze > requirements.txt` 命令生成。\n",
    "5. 实验报告要求 pdf 格式，要求包含姓名、学号。内容包括简要的**实验过程**和**关键代码**展示，对超参数的**实验分析**，最优超参数下的训练集、验证集**损失曲线**以及测试集上的**实验结果**。\n",
    "\n",
    "\n",
    "网络深度、学习率、网络宽度、激活函数\n",
    "\n",
    "## 实验设计\n",
    "实验整体设置为控制变量法，基准参数为width=5,depth=1,lr=0.1,activation=relu\n",
    "- 激活函数：设计激活函数分别为relu,sigmoid,tanh对比\n",
    "- 网络深度：设计网络深度分别为1,2,3,4对比\n",
    "- 网络宽度：设计网络宽度分别为1,5,10,20对比\n",
    "- 学习率：设计学习率分别为1,0.1,0.01,0.001对比\n",
    "\n",
    "## 实验结论\n",
    "- 网络深度：一般情况下深度越大网络性能越好，但是如果宽度较小增加深度对模型的改进有限\n",
    "- 学习率：学习率较低时学习速度较慢，但是学习率过高时可能找不到最优值\n",
    "- 网络宽度：一般情况下宽度越大网络参数越多性能越好\n",
    "- 激活函数：经过对比relu函数表现效果更好\n",
    "- 最终获得的最优参数结果为：width=5,depth=1,lr=0.1,activation=relu\n",
    "<table>\n",
    "  <tr>\n",
    "<img src=\"1.png\" alt=\"Image\" width=\"400\">\n",
    "<img src=\"2.png\" alt=\"Image\" width=\"400\">\n",
    "  </tr>\n",
    "  <tr>\n",
    "<img src=\"3.png\" alt=\"Image\" width=\"400\">\n",
    "<img src=\"4.png\" alt=\"Image\" width=\"400\">\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c73d1",
   "metadata": {},
   "source": [
    "## 1. 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37c9e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 2*np.pi, size=(3000,1))\n",
    "y = np.sin(x)\n",
    "\n",
    "x_train = torch.from_numpy(x[0 : 2400]).float()\n",
    "x_val = torch.from_numpy(x[2400 : 2700]).float()\n",
    "x_test = torch.from_numpy(x[2700 : 3000]).float()\n",
    "\n",
    "y_train = torch.from_numpy(x[0 : 2400]).float()\n",
    "y_val = torch.from_numpy(x[2400 : 2700]).float()\n",
    "y_test = torch.from_numpy(x[2700 : 3000]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea306824",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 模型搭建\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ed7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, activation = torch.sigmoid, layers = [1, 20, 1]):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.num_layers = len(layers)-1\n",
    "        self.fctions = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            self.fctions.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers-1):\n",
    "            x = self.activation(self.fctions[i](x))\n",
    "        x = self.fctions[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e94a9",
   "metadata": {},
   "source": [
    "## 3. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lambda function that returns the learning rate multiplier based on the current iteration/epoch\n",
    "def lr_lambda(current_step):\n",
    "    warmup_steps = 1000\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step/warmup_steps)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def Training(traindata , valdata, layers, activation, lr):\n",
    "    # create the network, optimizer, and loss function\n",
    "    net = Net(activation = activation, layers = layers)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
    "    loss_func = nn.MSELoss()\n",
    "    # Create a learning rate scheduler that uses the lambda function\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    # train the network\n",
    "    x_train, y_train = traindata\n",
    "    x_val, y_val = valdata\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    for i in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net(x_train)\n",
    "        loss_t = loss_func(y_pred, y_train)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_train.append(loss_t.item())\n",
    "        y_vpred = net(x_val)\n",
    "        loss_v = loss_func(y_vpred, y_val)\n",
    "        loss_val.append(loss_v.item())\n",
    "        \n",
    "    loss_curve = [loss_train, loss_val]\n",
    "    return net, loss_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d1b81",
   "metadata": {},
   "source": [
    "## 4. 性能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(activation=torch.sigmoid,depth=1,width=1,lr=0.1):\n",
    "    layers = [1]\n",
    "    for i in range(depth):\n",
    "        layers.append(width)\n",
    "    layers.append(1)\n",
    "    net, loss_curve = Training((x_train, y_train), (x_val, y_val), layers, activation, lr)\n",
    "    t = len(loss_curve[0])\n",
    "    y_tpred = net(x_test)\n",
    "    mse = nn.MSELoss()(y_tpred, y_test)\n",
    "    print(f\"训练集mse: {loss_curve[0][-1]}\\t验证集mse: {loss_curve[1][-1]}\\t测试集mse: {mse.item()}\")\n",
    "    return range(t), loss_curve, x_test, y_test, y_tpred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d063ad9",
   "metadata": {},
   "source": [
    "## 5. 调参分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9329e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(title,p):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4), dpi=300)\n",
    "    axs[0].set_title('loss_curve')\n",
    "    axs[1].set_title('scatter')\n",
    "    legend0 = []\n",
    "    legend1 = []\n",
    "    for i in range(len(title)):\n",
    "        axs[0].plot(p[i][0], p[i][1][0])\n",
    "        legend0.extend([title[i]+'_train'])\n",
    "        axs[1].scatter(p[i][2], p[i][3], s=1)\n",
    "        axs[1].scatter(p[i][2], p[i][4].detach().numpy(), s=1)\n",
    "        legend1.extend([title[i]+'_truth',title[i]+'_pred'])\n",
    "    axs[0].legend(legend0)\n",
    "    axs[1].legend(legend1)\n",
    "    plt.show()\n",
    "\n",
    "# 损失函数对比\n",
    "p11 = Test(activation=torch.sigmoid,depth=1,width=5,lr=0.1)\n",
    "p12 = Test(activation=torch.relu,depth=1,width=5,lr=0.1)\n",
    "p13 = Test(activation=torch.tanh,depth=1,width=5,lr=0.1)\n",
    "plotter(['sigmoid','relu','tanh'],[p11,p12,p13])\n",
    "\n",
    "# 深度对比\n",
    "p21 = Test(activation=torch.relu,depth=1,width=5,lr=0.1)\n",
    "p22 = Test(activation=torch.relu,depth=2,width=5,lr=0.1)\n",
    "p23 = Test(activation=torch.relu,depth=3,width=5,lr=0.1)\n",
    "p24 = Test(activation=torch.relu,depth=4,width=5,lr=0.1)\n",
    "plotter(['depth=1','depth=2','depth=3','depth=4'],[p21,p22,p23,p24])\n",
    "\n",
    "# 宽度对比\n",
    "p31 = Test(activation=torch.relu,depth=1,width=1,lr=0.1)\n",
    "p32 = Test(activation=torch.relu,depth=1,width=5,lr=0.1)\n",
    "p33 = Test(activation=torch.relu,depth=1,width=10,lr=0.1)\n",
    "p34 = Test(activation=torch.relu,depth=1,width=20,lr=0.1)\n",
    "plotter(['width=1','width=5','width=10','width=20'],[p31,p32,p33,p34])\n",
    "\n",
    "# 学习率对比\n",
    "p41 = Test(activation=torch.relu,depth=1,width=5,lr=0.1)\n",
    "p42 = Test(activation=torch.relu,depth=1,width=5,lr=0.01)\n",
    "p43 = Test(activation=torch.relu,depth=1,width=5,lr=0.001)\n",
    "p44 = Test(activation=torch.relu,depth=1,width=5,lr=1)\n",
    "plotter(['lr=0.1','lr=0.01','lr=0.01','lr=1'],[p41,p42,p43,p44])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
